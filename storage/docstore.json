{"docstore/metadata": {"ff120415-d091-4aba-af3b-5ef07e4acfe8": {"doc_hash": "294baa244ee74397976eb30119f9e22afd09f99aa46902a7bc5630da7005085b"}, "51867cd4-3fe9-4032-abba-482d2ec2a5af": {"doc_hash": "c9b72f3f0bee740da6317e89a6f7dcc62321d513ab017948902bf211ef27deb6"}, "d30862d0-3f80-4ee5-a0c8-f9675fd1d611": {"doc_hash": "5a210f96ae128085f624e05b3367781f4ef9a02648e240a6d389c83a9b10cca6"}, "45e734e1-7ebc-4b90-bd38-ca7f431f6c65": {"doc_hash": "7a7764463f0f71975f6dd0ab296803754ae7838330575b9581bb6ec76e1baf85"}, "9be60697-4e3e-4a43-b0c2-722eef472308": {"doc_hash": "7160e04a1bc9f7c98856bc3faab5df196707c817bda5cd73532a93308caf7cd9"}, "b54fdd17-312e-426d-bec6-9602ae4b50d2": {"doc_hash": "a7cdf2ef7f80a6ca03d218a9ba9ba6d0aa7e004a3459e70e3e3226086c200c2a"}, "7ce62fc2-852d-4b9e-82e6-9e09e27b7d4a": {"doc_hash": "1f559b6af1aeb191b732bad0f4d3ef7c1f1adc52a96e536924ecfaa1da20aa0e"}, "9a7586d2-94ad-4edf-bfae-fe267f8a4f58": {"doc_hash": "9c01b8468ff620bdface81b3fba5c7426c7939f8f72f7061514266025a4a053e"}, "07f60d65-1beb-412a-84d1-4819585ce3be": {"doc_hash": "1294a7884c696aa40d9d74ef12519007c2a511e416f34e05fce9ba537740502f"}, "7874f0d5-175f-4356-b007-f4f1582ddcb0": {"doc_hash": "5331b0db739e16afdc7f98cc00f43efcc9d86517c1dd9795f6cc8f5e101c466b"}, "1c939bf8-36c6-471f-beb2-804a4b9be401": {"doc_hash": "6f5ebda13504d376d02007c2044990bf5d1573859beaee97090e648b6b16916d"}, "4b024e4f-9516-4c4e-bea0-6f8d6fb97c4f": {"doc_hash": "5f5209ede952fb0a51c286b77a4cdc0c4db00096fe7e9a84e0c938698eff3b2f", "ref_doc_id": "ff120415-d091-4aba-af3b-5ef07e4acfe8"}, "5703dfa2-36f8-4bd9-aed9-461a67bfba2d": {"doc_hash": "f7f2ab41b5b6ac5b4bbe135ac28e7fabf14a185b4c7069117bce96570e116fbb", "ref_doc_id": "ff120415-d091-4aba-af3b-5ef07e4acfe8"}, "f82a26dd-9a89-473c-a434-3e9065e005d0": {"doc_hash": "ac9cd93410fd024aa6bcc7e6381dee4c0fb08db0b9516d304b2a5a0ee7d4fd84", "ref_doc_id": "51867cd4-3fe9-4032-abba-482d2ec2a5af"}, "5bbd3a55-1c50-45a2-8f04-73868fde21b3": {"doc_hash": "34ec26c6c94f72956e6ab25ddae32a7f3e12d034616eb6df8817bea7fe9d8220", "ref_doc_id": "51867cd4-3fe9-4032-abba-482d2ec2a5af"}, "ae445486-cbdf-4591-b90c-9e3627c1f240": {"doc_hash": "a73c937e7aaef79c0783d8596691771938be64378ea07e0d518e44632378b002", "ref_doc_id": "d30862d0-3f80-4ee5-a0c8-f9675fd1d611"}, "2a07ad9b-cc01-4e63-be9b-622d384053e1": {"doc_hash": "a80aee7b9c86ddf198cab3e6331d36b95b93fae619f7274bbfab289b443303ef", "ref_doc_id": "d30862d0-3f80-4ee5-a0c8-f9675fd1d611"}, "d49300ea-71ad-4833-a975-f52f92d82262": {"doc_hash": "da1e26e124e896dfa4b72f5f9671cd07778fee414a2a1ef75f0aa2a23044505b", "ref_doc_id": "45e734e1-7ebc-4b90-bd38-ca7f431f6c65"}, "56256e38-000c-40eb-9909-744850a328a0": {"doc_hash": "c5ec8b87a6ed10a88c0949806f5744976e42b96fd5fda9bb941e9506cbbc8080", "ref_doc_id": "45e734e1-7ebc-4b90-bd38-ca7f431f6c65"}, "f2dd1dd6-71f7-4c99-9c1b-3b44ef80881a": {"doc_hash": "168ec44a6b8e94d8d38019fe0333a83599abdc08fc80c6296566bece9918682f", "ref_doc_id": "45e734e1-7ebc-4b90-bd38-ca7f431f6c65"}, "3b443e15-85f4-48bf-a8a5-8da7273a185d": {"doc_hash": "55c8a9076d485cd0af9692f9305c4db8eb1814cbbc7df1e6cf9fc46fffb2636a", "ref_doc_id": "9be60697-4e3e-4a43-b0c2-722eef472308"}, "791cd122-ad36-465b-99d6-042e1a7b8358": {"doc_hash": "e404fb85bed16092e18a9796373553e10b008f3e1ed35e2efe85436ae7eba1e7", "ref_doc_id": "9be60697-4e3e-4a43-b0c2-722eef472308"}, "edc6e3cc-e431-4806-97a6-bbd33c4fd6ad": {"doc_hash": "c13aa14d322e38becd26c2c8365727a522284138728eaee0e8f05cfba246071a", "ref_doc_id": "b54fdd17-312e-426d-bec6-9602ae4b50d2"}, "c5f984af-ed55-453c-9193-63b39bfce16c": {"doc_hash": "b9bc1922d4a6017b503656b3d579216fe898140bcf80b79e415e419db0a75f49", "ref_doc_id": "b54fdd17-312e-426d-bec6-9602ae4b50d2"}, "9e5da7c9-3d7a-492c-8536-da97ff2d4a4d": {"doc_hash": "05d5f50bee4070ceb4cc0ddd505f20f146fbd5c612083cdb466aba4f8ff5bf2c", "ref_doc_id": "7ce62fc2-852d-4b9e-82e6-9e09e27b7d4a"}, "ebf3449a-18eb-4126-acfb-e9818a6013ea": {"doc_hash": "3ce1c4ea3ecce86b8620669058e0f6142d499ebd6b9bd659a98ab9a5fa6fcb21", "ref_doc_id": "7ce62fc2-852d-4b9e-82e6-9e09e27b7d4a"}, "01423a09-f7ee-4d22-b061-a0a3e7c87731": {"doc_hash": "1170069a23d6307bf32897f0e773bd88cf4dc542e5c7d5e165bcba356f628255", "ref_doc_id": "9a7586d2-94ad-4edf-bfae-fe267f8a4f58"}, "07907d11-6a94-4299-bc61-fd3146cd3589": {"doc_hash": "7c7612fd8cd1f0557fc036644118e903fec699aea646df85419416b6e3348bf0", "ref_doc_id": "9a7586d2-94ad-4edf-bfae-fe267f8a4f58"}, "cbae0c2f-3bd0-4689-872d-814147990287": {"doc_hash": "f16a93184a449c3a3c0c3b2590fb1f1a38c660a3ae585eb61599af66cea290ae", "ref_doc_id": "07f60d65-1beb-412a-84d1-4819585ce3be"}, "a4981ce5-8c19-48e8-8416-7381284b32d7": {"doc_hash": "d6e6f3737b3cf8192f13dfd07276ed2e4cb95ab7bee7d933ac5221ed932e82dc", "ref_doc_id": "07f60d65-1beb-412a-84d1-4819585ce3be"}, "1b897baa-9354-4eee-9132-5bf7e6f2d151": {"doc_hash": "a11f68e25f99ac357ac23bdc16fb5831ad398c4be67f575f0fd86dc323a0ab0f", "ref_doc_id": "7874f0d5-175f-4356-b007-f4f1582ddcb0"}, "6a2d5ae9-72f7-4331-b3aa-b6f8b4638445": {"doc_hash": "0ac6190a5540cade9abe0d6e637164e7a983784ef9cc6a7e814db43ff1fb093f", "ref_doc_id": "1c939bf8-36c6-471f-beb2-804a4b9be401"}}, "docstore/data": {"4b024e4f-9516-4c4e-bea0-6f8d6fb97c4f": {"__data__": {"id_": "4b024e4f-9516-4c4e-bea0-6f8d6fb97c4f", "embedding": null, "metadata": {"page_label": "1", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ff120415-d091-4aba-af3b-5ef07e4acfe8", "node_type": "4", "metadata": {"page_label": "1", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "294baa244ee74397976eb30119f9e22afd09f99aa46902a7bc5630da7005085b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5703dfa2-36f8-4bd9-aed9-461a67bfba2d", "node_type": "1", "metadata": {}, "hash": "d99ad25aff0dd24946a749b82663c273ff18c2461abcaa5f0c081d1607022885", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "arXiv:1502.03167v3  [cs.LG]  2 Mar 2015\nBatch Normalization: Accelerating Deep Network Training by\nReducing Internal Covariate Shift\nSergey Ioffe\nGoogle Inc.,sioffe@google.com\nChristian Szegedy\nGoogle Inc.,szegedy@google.com\nAbstract\nTraining Deep Neural Networks is complicated by the fact\nthat the distribution of each layer\u2019s inputs changes during\ntraining, as the parameters of the previous layers change.\nThis slows down the training by requiring lower learning\nrates and careful parameter initialization, and makes it no-\ntoriously hard to train models with saturating nonlineari-\nties. We refer to this phenomenon asinternal covariate\nshift, and address the problem by normalizing layer in-\nputs. Our method draws its strength from making normal-\nization a part of the model architecture and performing the\nnormalizationfor each training mini-batch. Batch Nor-\nmalization allows us to use much higher learning rates and\nbe less careful about initialization. It also acts as a regu-\nlarizer, in some cases eliminating the need for Dropout.\nApplied to a state-of-the-art image classi\ufb01cation model,\nBatch Normalization achieves the same accuracy with 14\ntimes fewer training steps, and beats the original model\nby a signi\ufb01cant margin. Using an ensemble of batch-\nnormalized networks, we improve upon the best published\nresult on ImageNet classi\ufb01cation: reaching 4.9% top-5\nvalidation error (and 4.8% test error), exceeding the ac-\ncuracy of human raters.\n1 Introduction\nDeep learning has dramatically advanced the state of the\nart in vision, speech, and many other areas. Stochas-\ntic gradient descent (SGD) has proved to be an effec-\ntive way of training deep networks, and SGD variants\nsuch as momentum (Sutskever et al., 2013) and Adagrad\n(Duchi et al., 2011) have been used to achieve state of the\nart performance. SGD optimizes the parameters\u0398 of the\nnetwork, so as to minimize the loss\n\u0398 = arg min\n\u0398\n1\nN\nN\u2211\ni=1\n\u2113(xi, \u0398)\nwhere x1...N is the training data set. With SGD, the train-\ning proceeds in steps, and at each step we consider amini-\nbatchx1...m of sizem. The mini-batch is used to approx-\nimate the gradient of the loss function with respect to the\nparameters, by computing\n1\nm\n\u2202\u2113(xi, \u0398)\n\u2202\u0398 .\nUsing mini-batches of examples, as opposed to one exam-\nple at a time, is helpful in several ways. First, the gradient\nof the loss over a mini-batch is an estimate of the gradient\nover the training set, whose quality improves as the batch\nsize increases. Second, computation over a batch can be\nmuch more ef\ufb01cient thanm computations for individual\nexamples, due to the parallelism afforded by the modern\ncomputing platforms.\nWhile stochastic gradient is simple and effective, it\nrequires careful tuning of the model hyper-parameters,\nspeci\ufb01cally the learning rate used in optimization, as well\nas the initial values for the model parameters. The train-\ning is complicated by the fact that the inputs to each layer\nare affected by the parameters of all preceding layers \u2013 so\nthat small changes to the network parameters amplify as\nthe network becomes deeper.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3066, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5703dfa2-36f8-4bd9-aed9-461a67bfba2d": {"__data__": {"id_": "5703dfa2-36f8-4bd9-aed9-461a67bfba2d", "embedding": null, "metadata": {"page_label": "1", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ff120415-d091-4aba-af3b-5ef07e4acfe8", "node_type": "4", "metadata": {"page_label": "1", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "294baa244ee74397976eb30119f9e22afd09f99aa46902a7bc5630da7005085b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b024e4f-9516-4c4e-bea0-6f8d6fb97c4f", "node_type": "1", "metadata": {"page_label": "1", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "5f5209ede952fb0a51c286b77a4cdc0c4db00096fe7e9a84e0c938698eff3b2f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The change in the distributions of layers\u2019 inputs\npresents a problem because the layers need to continu-\nously adapt to the new distribution. When the input dis-\ntribution to a learning system changes, it is said to experi-\nencecovariate shift(Shimodaira, 2000). This is typically\nhandled via domain adaptation (Jiang, 2008). However,\nthe notion of covariate shift can be extended beyond the\nlearning system as a whole, to apply to its parts, such as a\nsub-network or a layer. Consider a network computing\n\u2113 = F2(F1(u, \u0398 1), \u0398 2)\nwhere F1 and F2 are arbitrary transformations, and the\nparameters\u0398 1, \u0398 2 are to be learned so as to minimize\nthe loss\u2113. Learning\u0398 2 can be viewed as if the inputs\nx = F1(u, \u0398 1) are fed into the sub-network\n\u2113 = F2(x, \u0398 2).\nFor example, a gradient descent step\n\u0398 2 \u2190 \u0398 2 \u2212 \u03b1\nm\nm\u2211\ni=1\n\u2202F2(xi, \u0398 2)\n\u2202\u0398 2\n(for batch sizem and learning rate\u03b1) is exactly equivalent\nto that for a stand-alone networkF2 with inputx. There-\nfore, the input distribution properties that make training\nmore ef\ufb01cient \u2013 such as having the same distribution be-\ntween the training and test data \u2013 apply to training the\nsub-network as well. As such it is advantageous for the\ndistribution ofx to remain \ufb01xed over time. Then,\u0398 2 does\n1", "mimetype": "text/plain", "start_char_idx": 3067, "end_char_idx": 4301, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f82a26dd-9a89-473c-a434-3e9065e005d0": {"__data__": {"id_": "f82a26dd-9a89-473c-a434-3e9065e005d0", "embedding": null, "metadata": {"page_label": "2", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "51867cd4-3fe9-4032-abba-482d2ec2a5af", "node_type": "4", "metadata": {"page_label": "2", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "c9b72f3f0bee740da6317e89a6f7dcc62321d513ab017948902bf211ef27deb6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5bbd3a55-1c50-45a2-8f04-73868fde21b3", "node_type": "1", "metadata": {}, "hash": "14137843e12e75f7c8c4edc1efaa200a39e91896536033374c52b9a0efd89a49", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "not have to readjust to compensate for the change in the\ndistribution ofx.\nFixed distribution of inputs to a sub-network would\nhave positive consequences for the layersoutsidethe sub-\nnetwork, as well. Consider a layer with a sigmoid activa-\ntion functionz = g(W u + b) where u is the layer input,\nthe weight matrixW and bias vectorb are the layer pa-\nrameters to be learned, andg(x) = 1\n1+exp(\u2212x) . As |x|\nincreases,g\u2032(x) tends to zero. This means that for all di-\nmensions ofx = W u+b except those with small absolute\nvalues, the gradient \ufb02owing down tou will vanish and the\nmodel will train slowly. However, sincex is affected by\nW, b and the parameters of all the layers below, changes\nto those parameters during training will likely move many\ndimensions ofx into the saturated regime of the nonlin-\nearity and slow down the convergence. This effect is\nampli\ufb01ed as the network depth increases. In practice,\nthe saturation problem and the resulting vanishing gradi-\nents are usually addressed by using Recti\ufb01ed Linear Units\n(Nair & Hinton, 2010)ReLU (x) = max( x, 0), careful\ninitialization (Bengio & Glorot, 2010; Saxe et al., 2013),\nand small learning rates. If, however, we could ensure\nthat the distribution of nonlinearity inputs remains more\nstable as the network trains, then the optimizer would be\nless likely to get stuck in the saturated regime, and the\ntraining would accelerate.\nWe refer to the change in the distributions of internal\nnodes of a deep network, in the course of training, asIn-\nternal Covariate Shift. Eliminating it offers a promise of\nfaster training. We propose a new mechanism, which we\ncallBatch Normalization, that takes a step towards re-\nducing internal covariate shift, and in doing so dramati-\ncally accelerates the training of deep neural nets. It ac-\ncomplishes this via a normalization step that \ufb01xes the\nmeans and variances of layer inputs. Batch Normalization\nalso has a bene\ufb01cial effect on the gradient \ufb02ow through\nthe network, by reducing the dependence of gradients\non the scale of the parameters or of their initial values.\nThis allows us to use much higher learning rates with-\nout the risk of divergence. Furthermore, batch normal-\nization regularizes the model and reduces the need for\nDropout (Srivastava et al., 2014). Finally, Batch Normal-\nization makes it possible to use saturating nonlinearities\nby preventing the network from getting stuck in the satu-\nrated modes.\nIn Sec. 4.2, we apply Batch Normalization to the best-\nperforming ImageNet classi\ufb01cation network, and show\nthat we can match its performance using only 7% of the\ntraining steps, and can further exceed its accuracy by a\nsubstantial margin. Using an ensemble of such networks\ntrained with Batch Normalization, we achieve the top-5\nerror rate that improves upon the best known results on\nImageNet classi\ufb01cation.\n2 Towards Reducing Internal\nCovariate Shift\nWe de\ufb01ne Internal Covariate Shiftas the change in the\ndistribution of network activations due to the change in\nnetwork parameters during training. To improve the train-\ning, we seek to reduce the internal covariate shift.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3100, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5bbd3a55-1c50-45a2-8f04-73868fde21b3": {"__data__": {"id_": "5bbd3a55-1c50-45a2-8f04-73868fde21b3", "embedding": null, "metadata": {"page_label": "2", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "51867cd4-3fe9-4032-abba-482d2ec2a5af", "node_type": "4", "metadata": {"page_label": "2", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "c9b72f3f0bee740da6317e89a6f7dcc62321d513ab017948902bf211ef27deb6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f82a26dd-9a89-473c-a434-3e9065e005d0", "node_type": "1", "metadata": {"page_label": "2", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "ac9cd93410fd024aa6bcc7e6381dee4c0fb08db0b9516d304b2a5a0ee7d4fd84", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To improve the train-\ning, we seek to reduce the internal covariate shift. By\n\ufb01xing the distribution of the layer inputsx as the training\nprogresses, we expect to improve the training speed. It has\nbeen long known (LeCun et al., 1998b; Wiesler & Ney,\n2011) that the network training converges faster if its in-\nputs are whitened \u2013 i.e., linearly transformed to have zero\nmeans and unit variances, and decorrelated. As each layer\nobserves the inputs produced by the layers below, it would\nbe advantageous to achieve the same whitening of the in-\nputs of each layer. By whitening the inputs to each layer,\nwe would take a step towards achieving the \ufb01xed distri-\nbutions of inputs that would remove the ill effects of the\ninternal covariate shift.\nWe could consider whitening activations at every train-\ning step or at some interval, either by modifying the\nnetwork directly or by changing the parameters of the\noptimization algorithm to depend on the network ac-\ntivation values (Wiesler et al., 2014; Raiko et al., 2012;\nPovey et al., 2014; Desjardins & Kavukcuoglu). How-\never, if these modi\ufb01cations are interspersed with the op-\ntimization steps, then the gradient descent step may at-\ntempt to update the parameters in a way that requires\nthe normalization to be updated, which reduces the ef-\nfect of the gradient step. For example, consider a layer\nwith the inputu that adds the learned biasb, and normal-\nizes the result by subtracting the mean of the activation\ncomputed over the training data:\u02c6x = x \u2212 E[x] where\nx = u + b, X = {x1...N }is the set of values ofx over\nthe training set, and E[x] = 1\nN\n\u2211 N\ni=1 xi. If a gradient\ndescent step ignores the dependence of E[x] on b, then it\nwill updateb \u2190 b + \u2206 b, where\u2206 b \u221d\u2212 \u2202\u2113/\u2202 \u02c6x. Then\nu + ( b + \u2206 b) \u2212 E[u + ( b + \u2206 b)] = u + b \u2212 E[u + b].\nThus, the combination of the update tob and subsequent\nchange in normalization led to no change in the output\nof the layer nor, consequently, the loss. As the training\ncontinues,b will grow inde\ufb01nitely while the loss remains\n\ufb01xed. This problem can get worse if the normalization not\nonly centers but also scales the activations. We have ob-\nserved this empirically in initial experiments, where the\nmodel blows up when the normalization parameters are\ncomputed outside the gradient descent step.\nThe issue with the above approach is that the gradient\ndescent optimization does not take into account the fact\nthat the normalization takes place. To address this issue,\nwe would like to ensure that, for any parameter values,\nthe networkalwaysproduces activations with the desired\ndistribution. Doing so would allow the gradient of the\nloss with respect to the model parameters to account for\nthe normalization, and for its dependence on the model\nparameters\u0398 . Let againx be a layer input, treated as a\n2", "mimetype": "text/plain", "start_char_idx": 3026, "end_char_idx": 5824, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ae445486-cbdf-4591-b90c-9e3627c1f240": {"__data__": {"id_": "ae445486-cbdf-4591-b90c-9e3627c1f240", "embedding": null, "metadata": {"page_label": "3", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d30862d0-3f80-4ee5-a0c8-f9675fd1d611", "node_type": "4", "metadata": {"page_label": "3", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "5a210f96ae128085f624e05b3367781f4ef9a02648e240a6d389c83a9b10cca6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a07ad9b-cc01-4e63-be9b-622d384053e1", "node_type": "1", "metadata": {}, "hash": "81ad238b3de5fb631aabb72be2657166f79f013f7cc2f25bab52b2bb165b77be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "vector, andXbe the set of these inputs over the training\ndata set. The normalization can then be written as a trans-\nformation\n\u02c6x = Norm (x, X)\nwhich depends not only on the given training examplex\nbut on all examplesX\u2013 each of which depends on\u0398 if\nx is generated by another layer. For backpropagation, we\nwould need to compute the Jacobians\n\u2202Norm (x, X)\n\u2202x and \u2202Norm (x, X)\n\u2202X ;\nignoring the latter term would lead to the explosion de-\nscribed above. Within this framework, whitening the layer\ninputs is expensive, as it requires computing the covari-\nance matrix Cov[x] = Ex\u2208X [xxT ] \u2212 E[x]E[x]T and its\ninverse square root, to produce the whitened activations\nCov[x]\u22121/2(x \u2212 E[x]), as well as the derivatives of these\ntransforms for backpropagation. This motivates us to seek\nan alternative that performs input normalization in a way\nthat is differentiable and does not require the analysis of\nthe entire training set after every parameter update.\nSome of the previous approaches (e.g.\n(Lyu & Simoncelli, 2008)) use statistics computed\nover a single training example, or, in the case of image\nnetworks, over different feature maps at a given location.\nHowever, this changes the representation ability of a\nnetwork by discarding the absolute scale of activations.\nWe want to a preserve the information in the network, by\nnormalizing the activations in a training example relative\nto the statistics of the entire training data.\n3 Normalization via Mini-Batch\nStatistics\nSince the full whitening of each layer\u2019s inputs is costly\nand not everywhere differentiable, we make two neces-\nsary simpli\ufb01cations. The \ufb01rst is that instead of whitening\nthe features in layer inputs and outputs jointly, we will\nnormalize each scalar feature independently, by making it\nhave the mean of zero and the variance of 1. For a layer\nwithd-dimensional inputx = ( x(1) . . . x (d)), we will nor-\nmalize each dimension\n\u02c6x(k) = x(k) \u2212E[x(k)]\n\u221a\nVar[x(k)]\nwhere the expectation and variance are computed over the\ntraining data set. As shown in (LeCun et al., 1998b), such\nnormalization speeds up convergence, even when the fea-\ntures are not decorrelated.\nNote that simply normalizing each input of a layer may\nchange what the layer can represent. For instance, nor-\nmalizing the inputs of a sigmoid would constrain them to\nthe linear regime of the nonlinearity. To address this, we\nmake sure thatthe transformation inserted in the network\ncan represent the identity transform. To accomplish this,\nwe introduce, for each activationx(k), a pair of parameters\n\u03b3(k), \u03b2(k), which scale and shift the normalized value:\ny(k) = \u03b3(k)\u02c6x(k) + \u03b2(k).\nThese parameters are learned along with the original\nmodel parameters, and restore the representation power\nof the network. Indeed, by setting\u03b3(k) =\n\u221a\nVar[x(k)] and\n\u03b2(k) = E[x(k)], we could recover the original activations,\nif that were the optimal thing to do.\nIn the batch setting where each training step is based on\nthe entire training set, we would use the whole set to nor-\nmalize activations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3014, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2a07ad9b-cc01-4e63-be9b-622d384053e1": {"__data__": {"id_": "2a07ad9b-cc01-4e63-be9b-622d384053e1", "embedding": null, "metadata": {"page_label": "3", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d30862d0-3f80-4ee5-a0c8-f9675fd1d611", "node_type": "4", "metadata": {"page_label": "3", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "5a210f96ae128085f624e05b3367781f4ef9a02648e240a6d389c83a9b10cca6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae445486-cbdf-4591-b90c-9e3627c1f240", "node_type": "1", "metadata": {"page_label": "3", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "a73c937e7aaef79c0783d8596691771938be64378ea07e0d518e44632378b002", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, this is impractical when us-\ning stochastic optimization. Therefore, we make the sec-\nond simpli\ufb01cation: since we use mini-batches in stochas-\ntic gradient training,each mini-batch produces estimates\nof the mean and varianceof each activation. This way, the\nstatistics used for normalization can fully participate in\nthe gradient backpropagation. Note that the use of mini-\nbatches is enabled by computation of per-dimension vari-\nances rather than joint covariances; in the joint case, reg-\nularization would be required since the mini-batch size is\nlikely to be smaller than the number of activations being\nwhitened, resulting in singular covariance matrices.\nConsider a mini-batchBof sizem. Since the normal-\nization is applied to each activation independently, let us\nfocus on a particular activationx(k) and omitk for clarity.\nWe have m values of this activation in the mini-batch,\nB= {x1...m}.\nLet the normalized values be\u02c6x1...m, and their linear trans-\nformations bey1...m. We refer to the transform\nBN \u03b3,\u03b2 : x1...m \u2192 y1...m\nas theBatch Normalizing Transform. We present the BN\nTransform in Algorithm 1. In the algorithm,\u01eb is a constant\nadded to the mini-batch variance for numerical stability.\nInput: Values ofx over a mini-batch:B= {x1...m};\nParameters to be learned:\u03b3,\u03b2\nOutput: {yi = BN \u03b3,\u03b2 (xi)}\n\u00b5B \u2190 1\nm\nm\u2211\ni=1\nxi // mini-batch mean\n\u03c32\nB \u2190 1\nm\nm\u2211\ni=1\n(xi \u2212\u00b5B)2 // mini-batch variance\n\u02c6xi \u2190 xi \u2212\u00b5B\u221a\n\u03c32\nB + \u01eb\n// normalize\nyi \u2190 \u03b3\u02c6xi + \u03b2 \u2261 BN \u03b3,\u03b2 (xi) // scale and shift\nAlgorithm 1:Batch Normalizing Transform, applied to\nactivationx over a mini-batch.\nThe BN transform can be added to a network to manip-\nulate any activation. In the notationy = BN \u03b3,\u03b2 (x), we\n3", "mimetype": "text/plain", "start_char_idx": 3015, "end_char_idx": 4697, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d49300ea-71ad-4833-a975-f52f92d82262": {"__data__": {"id_": "d49300ea-71ad-4833-a975-f52f92d82262", "embedding": null, "metadata": {"page_label": "4", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45e734e1-7ebc-4b90-bd38-ca7f431f6c65", "node_type": "4", "metadata": {"page_label": "4", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "7a7764463f0f71975f6dd0ab296803754ae7838330575b9581bb6ec76e1baf85", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56256e38-000c-40eb-9909-744850a328a0", "node_type": "1", "metadata": {}, "hash": "b4126ad49d6427e2b363fd3683e18027e859f0d9d5196dde7c5619772c16bf25", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "indicate that the parameters\u03b3 and \u03b2 are to be learned,\nbut it should be noted that the BN transform does not\nindependently process the activation in each training ex-\nample. Rather, BN\u03b3,\u03b2 (x) depends both on the training\nexample and the other examples in the mini-batch. The\nscaled and shifted valuesy are passed to other network\nlayers. The normalized activations\u02c6x are internal to our\ntransformation, but their presence is crucial. The distri-\nbutions of values of any\u02c6x has the expected value of0\nand the variance of1, as long as the elements of each\nmini-batch are sampled from the same distribution, and\nif we neglect\u01eb. This can be seen by observing that\u2211 m\ni=1 \u02c6xi = 0 and 1\nm\n\u2211 m\ni=1 \u02c6x2\ni = 1 , and taking expec-\ntations. Each normalized activation\u02c6x(k) can be viewed as\nan input to a sub-network composed of the linear trans-\nformy(k) = \u03b3(k)\u02c6x(k) + \u03b2(k), followed by the other pro-\ncessing done by the original network. These sub-network\ninputs all have \ufb01xed means and variances, and although\nthe joint distribution of these normalized\u02c6x(k) can change\nover the course of training, we expect that the introduc-\ntion of normalized inputs accelerates the training of the\nsub-network and, consequently, the network as a whole.\nDuring training we need to backpropagate the gradi-\nent of loss\u2113 through this transformation, as well as com-\npute the gradients with respect to the parameters of the\nBN transform. We use chain rule, as follows (before sim-\npli\ufb01cation):\n\u2202\u2113\n\u2202\u02c6xi\n= \u2202\u2113\n\u2202yi\n\u00b7\u03b3\n\u2202\u2113\n\u2202\u03c32\nB\n= \u2211 m\ni=1\n\u2202\u2113\n\u2202\u02c6xi\n\u00b7(xi \u2212\u00b5B) \u00b7\u22121\n2 (\u03c32\nB + \u01eb)\u22123/2\n\u2202\u2113\n\u2202\u00b5B\n=\n( \u2211 m\ni=1\n\u2202\u2113\n\u2202\u02c6xi\n\u00b7 \u22121\u221a\n\u03c32\nB +\u01eb\n)\n+ \u2202\u2113\n\u2202\u03c32\nB\n\u00b7\n\u2211 m\ni=1 \u22122(xi\u2212\u00b5B )\nm\n\u2202\u2113\n\u2202xi\n= \u2202\u2113\n\u2202\u02c6xi\n\u00b7 1\u221a\n\u03c32\nB+\u01eb + \u2202\u2113\n\u2202\u03c32\nB\n\u00b72(xi\u2212\u00b5B)\nm + \u2202\u2113\n\u2202\u00b5B\n\u00b71\nm\n\u2202\u2113\n\u2202\u03b3 = \u2211 m\ni=1\n\u2202\u2113\n\u2202yi\n\u00b7\u02c6xi\n\u2202\u2113\n\u2202\u03b2 = \u2211 m\ni=1\n\u2202\u2113\n\u2202yi\nThus, BN transform is a differentiable transformation that\nintroduces normalized activations into the network. This\nensures that as the model is training, layers can continue\nlearning on input distributions that exhibit less internalco-\nvariate shift, thus accelerating the training.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2024, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "56256e38-000c-40eb-9909-744850a328a0": {"__data__": {"id_": "56256e38-000c-40eb-9909-744850a328a0", "embedding": null, "metadata": {"page_label": "4", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45e734e1-7ebc-4b90-bd38-ca7f431f6c65", "node_type": "4", "metadata": {"page_label": "4", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "7a7764463f0f71975f6dd0ab296803754ae7838330575b9581bb6ec76e1baf85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d49300ea-71ad-4833-a975-f52f92d82262", "node_type": "1", "metadata": {"page_label": "4", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "da1e26e124e896dfa4b72f5f9671cd07778fee414a2a1ef75f0aa2a23044505b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2dd1dd6-71f7-4c99-9c1b-3b44ef80881a", "node_type": "1", "metadata": {}, "hash": "fb3a90be1da862bad92f3260e288bafcc5d28d89ef82076bf04e56fba2fbf801", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Furthermore,\nthe learned af\ufb01ne transform applied to these normalized\nactivations allows the BN transform to represent the iden-\ntity transformation and preserves the network capacity.\n3.1 Training and Inference with Batch-\nNormalized Networks\nTo Batch-Normalizea network, we specify a subset of ac-\ntivations and insert the BN transform for each of them,\naccording to Alg. 1. Any layer that previously received\nxas the input, now receives BN(x). A model employing\nBatch Normalization can be trained using batch gradient\ndescent, or Stochastic Gradient Descent with a mini-batch\nsizem > 1, or with any of its variants such as Adagrad\n(Duchi et al., 2011). The normalization of activations that\ndepends on the mini-batch allows ef\ufb01cient training, but is\nneither necessary nor desirable during inference; we want\nthe output to depend only on the input, deterministically.\nFor this, once the network has been trained, we use the\nnormalization\n\u02c6x = x \u2212E[x]\n\u221a\nVar[x] + \u01eb\nusing the population, rather than mini-batch, statistics.\nNeglecting\u01eb, these normalized activations have the same\nmean 0 and variance 1 as during training. We use the un-\nbiased variance estimate Var[x] = m\nm\u22121 \u00b7EB[\u03c32\nB], where\nthe expectation is over training mini-batches of sizem and\n\u03c32\nB are their sample variances. Using moving averages in-\nstead, we can track the accuracy of a model as it trains.\nSince the means and variances are \ufb01xed during inference,\nthe normalization is simply a linear transform applied to\neach activation. It may further be composed with the scal-\ning by\u03b3 and shift by\u03b2, to yield a single linear transform\nthat replaces BN(x). Algorithm 2 summarizes the proce-\ndure for training batch-normalized networks.\nInput: Network N with trainable parameters\u0398 ;\nsubset of activations{x(k)}K\nk=1\nOutput: Batch-normalized network for inference,\nN inf\nBN\n1:\nN tr\nBN\u2190\nN // Training BN network\n2: fork = 1 . . . K do\n3: Add transformationy(k) = BN \u03b3(k),\u03b2(k) (x(k)) to\nN tr\nBN(Alg. 1)\n4: Modify each layer in\nN tr\nBNwith inputx(k) to take\ny(k) instead\n5: end for\n6: Train\nN tr\nBN to optimize the parameters \u0398 \u222a\n{\u03b3(k), \u03b2(k)}K\nk=1\n7:\nN inf\nBN\u2190\nN tr\nBN // Inference BN network with frozen\n// parameters\n8: fork = 1 . . . K do\n9: // For clarity,x \u2261 x(k), \u03b3 \u2261 \u03b3(k), \u00b5B \u2261 \u00b5(k)\nB , etc.", "mimetype": "text/plain", "start_char_idx": 2025, "end_char_idx": 4283, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f2dd1dd6-71f7-4c99-9c1b-3b44ef80881a": {"__data__": {"id_": "f2dd1dd6-71f7-4c99-9c1b-3b44ef80881a", "embedding": null, "metadata": {"page_label": "4", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45e734e1-7ebc-4b90-bd38-ca7f431f6c65", "node_type": "4", "metadata": {"page_label": "4", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "7a7764463f0f71975f6dd0ab296803754ae7838330575b9581bb6ec76e1baf85", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56256e38-000c-40eb-9909-744850a328a0", "node_type": "1", "metadata": {"page_label": "4", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "c5ec8b87a6ed10a88c0949806f5744976e42b96fd5fda9bb941e9506cbbc8080", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10: Process multiple training mini-batchesB, each of\nsizem, and average over them:\nE[x] \u2190 EB[\u00b5B]\nVar[x] \u2190 m\nm\u22121 EB[\u03c32\nB]\n11: InN inf\nBN, replace the transformy = BN \u03b3,\u03b2 (x) with\ny = \u03b3\n\u221a\nVar[x]+\u01eb\n\u00b7x +\n(\n\u03b2 \u2212 \u03b3 E[x]\u221a\nVar[x]+\u01eb\n)\n12: end for\nAlgorithm 2:Training a Batch-Normalized Network\n3.2 Batch-Normalized Convolutional Net-\nworks\nBatch Normalization can be applied to any set of acti-\nvations in the network. Here, we focus on transforms\n4", "mimetype": "text/plain", "start_char_idx": 4284, "end_char_idx": 4724, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3b443e15-85f4-48bf-a8a5-8da7273a185d": {"__data__": {"id_": "3b443e15-85f4-48bf-a8a5-8da7273a185d", "embedding": null, "metadata": {"page_label": "5", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9be60697-4e3e-4a43-b0c2-722eef472308", "node_type": "4", "metadata": {"page_label": "5", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "7160e04a1bc9f7c98856bc3faab5df196707c817bda5cd73532a93308caf7cd9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "791cd122-ad36-465b-99d6-042e1a7b8358", "node_type": "1", "metadata": {}, "hash": "a64411edbe978dda0f23228733f0877c67fcdc4d84d592c34575815b77a43495", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "that consist of an af\ufb01ne transformation followed by an\nelement-wise nonlinearity:\nz = g(W u + b)\nwhere W and b are learned parameters of the model, and\ng(\u00b7) is the nonlinearity such as sigmoid or ReLU. This for-\nmulation covers both fully-connected and convolutional\nlayers. We add the BN transform immediately before the\nnonlinearity, by normalizingx = W u + b. We could have\nalso normalized the layer inputsu, but sinceu is likely\nthe output of another nonlinearity, the shape of its distri-\nbution is likely to change during training, and constraining\nits \ufb01rst and second moments would not eliminate the co-\nvariate shift. In contrast,W u + b is more likely to have\na symmetric, non-sparse distribution, that is \u201cmore Gaus-\nsian\u201d (Hyv\u00a8 arinen & Oja, 2000); normalizing it is likely to\nproduce activations with a stable distribution.\nNote that, since we normalizeW u+b, the biasb can be\nignored since its effect will be canceled by the subsequent\nmean subtraction (the role of the bias is subsumed by\u03b2 in\nAlg. 1). Thus,z = g(W u + b) is replaced with\nz = g(BN (W u))\nwhere the BN transform is applied independently to each\ndimension ofx = W u, with a separate pair of learned\nparameters\u03b3(k),\u03b2(k) per dimension.\nFor convolutional layers, we additionally want the nor-\nmalization to obey the convolutional property \u2013 so that\ndifferent elements of the same feature map, at different\nlocations, are normalized in the same way. To achieve\nthis, we jointly normalize all the activations in a mini-\nbatch, over all locations. In Alg. 1, we letBbe the set of\nall values in a feature map across both the elements of a\nmini-batch and spatial locations \u2013 so for a mini-batch of\nsizem and feature maps of sizep \u00d7 q, we use the effec-\ntive mini-batch of sizem\u2032 = |B|= m \u00b7p q. We learn a\npair of parameters\u03b3(k) and \u03b2(k) per feature map, rather\nthan per activation. Alg. 2 is modi\ufb01ed similarly, so that\nduring inference the BN transform applies the same linear\ntransformation to each activation in a given feature map.\n3.3 Batch Normalization enables higher\nlearning rates\nIn traditional deep networks, too-high learning rate may\nresult in the gradients that explode or vanish, as well as\ngetting stuck in poor local minima. Batch Normaliza-\ntion helps address these issues. By normalizing activa-\ntions throughout the network, it prevents small changes\nto the parameters from amplifying into larger and subop-\ntimal changes in activations in gradients; for instance, it\nprevents the training from getting stuck in the saturated\nregimes of nonlinearities.\nBatch Normalization also makes training more resilient\nto the parameter scale. Normally, large learning rates may\nincrease the scale of layer parameters, which then amplify\nthe gradient during backpropagation and lead to the model\nexplosion. However, with Batch Normalization, back-\npropagation through a layer is unaffected by the scale of\nits parameters.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2899, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "791cd122-ad36-465b-99d6-042e1a7b8358": {"__data__": {"id_": "791cd122-ad36-465b-99d6-042e1a7b8358", "embedding": null, "metadata": {"page_label": "5", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9be60697-4e3e-4a43-b0c2-722eef472308", "node_type": "4", "metadata": {"page_label": "5", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "7160e04a1bc9f7c98856bc3faab5df196707c817bda5cd73532a93308caf7cd9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b443e15-85f4-48bf-a8a5-8da7273a185d", "node_type": "1", "metadata": {"page_label": "5", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "55c8a9076d485cd0af9692f9305c4db8eb1814cbbc7df1e6cf9fc46fffb2636a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Indeed, for a scalara,\nBN (W u) = BN ((aW )u)\nand we can show that\n\u2202BN ((aW )u)\n\u2202u = \u2202BN (W u)\n\u2202u\n\u2202BN ((aW )u)\n\u2202(aW ) = 1\na \u00b7\u2202BN (W u)\n\u2202W\nThe scale does not affect the layer Jacobian nor, con-\nsequently, the gradient propagation. Moreover, larger\nweights lead tosmallergradients, and Batch Normaliza-\ntion will stabilize the parameter growth.\nWe further conjecture that Batch Normalization may\nlead the layer Jacobians to have singular values close to 1,\nwhich is known to be bene\ufb01cial for training (Saxe et al.,\n2013). Consider two consecutive layers with normalized\ninputs, and the transformation between these normalized\nvectors:\u02c6z = F (\u02c6x). If we assume that\u02c6x and\u02c6z are Gaussian\nand uncorrelated, and thatF (\u02c6x) \u2248 J\u02c6x is a linear transfor-\nmation for the given model parameters, then both\u02c6x and \u02c6z\nhave unit covariances, andI = Cov [\u02c6z] = JCov [\u02c6x]JT =\nJJ T . Thus,JJ T = I, and so all singular values ofJ\nare equal to 1, which preserves the gradient magnitudes\nduring backpropagation. In reality, the transformation is\nnot linear, and the normalized values are not guaranteed to\nbe Gaussian nor independent, but we nevertheless expect\nBatch Normalization to help make gradient propagation\nbetter behaved. The precise effect of Batch Normaliza-\ntion on gradient propagation remains an area of further\nstudy.\n3.4 Batch Normalization regularizes the\nmodel\nWhen training with Batch Normalization, a training ex-\nample is seen in conjunction with other examples in the\nmini-batch, and the training network no longer produc-\ning deterministic values for a given training example. In\nour experiments, we found this effect to be advantageous\nto the generalization of the network. Whereas Dropout\n(Srivastava et al., 2014) is typically used to reduce over-\n\ufb01tting, in a batch-normalized network we found that it can\nbe either removed or reduced in strength.\n4 Experiments\n4.1 Activations over time\nTo verify the effects of internal covariate shift on train-\ning, and the ability of Batch Normalization to combat it,\nwe considered the problem of predicting the digit class on\nthe MNIST dataset (LeCun et al., 1998a). We used a very\nsimple network, with a 28x28 binary image as input, and\n5", "mimetype": "text/plain", "start_char_idx": 2900, "end_char_idx": 5085, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "edc6e3cc-e431-4806-97a6-bbd33c4fd6ad": {"__data__": {"id_": "edc6e3cc-e431-4806-97a6-bbd33c4fd6ad", "embedding": null, "metadata": {"page_label": "6", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b54fdd17-312e-426d-bec6-9602ae4b50d2", "node_type": "4", "metadata": {"page_label": "6", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "a7cdf2ef7f80a6ca03d218a9ba9ba6d0aa7e004a3459e70e3e3226086c200c2a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5f984af-ed55-453c-9193-63b39bfce16c", "node_type": "1", "metadata": {}, "hash": "5cb7baee1a899d3b3c45e0e4e6952c8103e066dfd459c2f5ffc7acd05f9a4a7e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10K 20K 30K 40K 50K\n0.7\n0.8\n0.9\n1\n \n \nWithout BN\nWith BN\n\u22122\n0\n2\n\u22122\n0\n2\n(a) (b) Without BN (c) With BN\nFigure 1: (a)The test accuracy of the MNIST network\ntrained with and without Batch Normalization, vs. the\nnumber of training steps. Batch Normalization helps the\nnetwork train faster and achieve higher accuracy.(b,\nc) The evolution of input distributions to a typical sig-\nmoid, over the course of training, shown as{15, 50, 85}th\npercentiles. Batch Normalization makes the distribution\nmore stable and reduces the internal covariate shift.\n3 fully-connected hidden layers with 100 activations each.\nEach hidden layer computesy = g(W u+b) with sigmoid\nnonlinearity, and the weightsW initialized to small ran-\ndom Gaussian values. The last hidden layer is followed\nby a fully-connected layer with 10 activations (one per\nclass) and cross-entropy loss. We trained the network for\n50000 steps, with 60 examples per mini-batch. We added\nBatch Normalization to each hidden layer of the network,\nas in Sec. 3.1. We were interested in the comparison be-\ntween the baseline and batch-normalized networks, rather\nthan achieving the state of the art performance on MNIST\n(which the described architecture does not).\nFigure 1(a) shows the fraction of correct predictions\nby the two networks on held-out test data, as training\nprogresses. The batch-normalized network enjoys the\nhigher test accuracy. To investigate why, we studied in-\nputs to the sigmoid, in the original network\nN and batch-\nnormalized networkN tr\nBN(Alg. 2) over the course of train-\ning. In Fig. 1(b,c) we show, for one typical activation from\nthe last hidden layer of each network, how its distribu-\ntion evolves. The distributions in the original network\nchange signi\ufb01cantly over time, both in their mean and\nthe variance, which complicates the training of the sub-\nsequent layers. In contrast, the distributions in the batch-\nnormalized network are much more stable as training pro-\ngresses, which aids the training.\n4.2 ImageNet classi\ufb01cation\nWe applied Batch Normalization to a new variant of the\nInception network (Szegedy et al., 2014), trained on the\nImageNet classi\ufb01cation task (Russakovsky et al., 2014).\nThe network has a large number of convolutional and\npooling layers, with a softmax layer to predict the image\nclass, out of 1000 possibilities. Convolutional layers use\nReLU as the nonlinearity. The main difference to the net-\nwork described in (Szegedy et al., 2014) is that the5 \u00d7 5\nconvolutional layers are replaced by two consecutive lay-\ners of3 \u00d7 3 convolutions with up to128 \ufb01lters. The net-\nwork contains13.6 \u00b7106 parameters, and, other than the\ntop softmax layer, has no fully-connected layers. More\ndetails are given in the Appendix. We refer to this model\nas\nInceptionin the rest of the text.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2779, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c5f984af-ed55-453c-9193-63b39bfce16c": {"__data__": {"id_": "c5f984af-ed55-453c-9193-63b39bfce16c", "embedding": null, "metadata": {"page_label": "6", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b54fdd17-312e-426d-bec6-9602ae4b50d2", "node_type": "4", "metadata": {"page_label": "6", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "a7cdf2ef7f80a6ca03d218a9ba9ba6d0aa7e004a3459e70e3e3226086c200c2a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "edc6e3cc-e431-4806-97a6-bbd33c4fd6ad", "node_type": "1", "metadata": {"page_label": "6", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "c13aa14d322e38becd26c2c8365727a522284138728eaee0e8f05cfba246071a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We refer to this model\nas\nInceptionin the rest of the text. The model was trained\nusing a version of Stochastic Gradient Descent with mo-\nmentum (Sutskever et al., 2013), using the mini-batch size\nof 32. The training was performed using a large-scale, dis-\ntributed architecture (similar to (Dean et al., 2012)). All\nnetworks are evaluated as training progresses by comput-\ning the validation accuracy@1, i.e. the probability of\npredicting the correct label out of 1000 possibilities, on\na held-out set, using a single crop per image.\nIn our experiments, we evaluated several modi\ufb01cations\nof Inception with Batch Normalization. In all cases, Batch\nNormalization was applied to the input of each nonlinear-\nity, in a convolutional way, as described in section 3.2,\nwhile keeping the rest of the architecture constant.\n4.2.1 Accelerating BN Networks\nSimply adding Batch Normalization to a network does not\ntake full advantage of our method. To do so, we further\nchanged the network and its training parameters, as fol-\nlows:\nIncrease learning rate.In a batch-normalized model,\nwe have been able to achieve a training speedup from\nhigher learning rates, with no ill side effects (Sec. 3.3).\nRemove Dropout.As described in Sec. 3.4, Batch Nor-\nmalization ful\ufb01lls some of the same goals as Dropout. Re-\nmoving Dropout from Modi\ufb01ed BN-Inception speeds up\ntraining, without increasing over\ufb01tting.\nReduce theL2 weight regularization.While in Incep-\ntion anL2 loss on the model parameters controls over\ufb01t-\nting, in Modi\ufb01ed BN-Inception the weight of this loss is\nreduced by a factor of 5. We \ufb01nd that this improves the\naccuracy on the held-out validation data.\nAccelerate the learning rate decay.In training Incep-\ntion, learning rate was decayed exponentially. Because\nour network trains faster than Inception, we lower the\nlearning rate 6 times faster.\nRemove Local Response NormalizationWhile Incep-\ntion and other networks (Srivastava et al., 2014) bene\ufb01t\nfrom it, we found that with Batch Normalization it is not\nnecessary.\nShuf\ufb02e training examples more thoroughly.We enabled\nwithin-shard shuf\ufb02ing of the training data, which prevents\nthe same examples from always appearing in a mini-batch\ntogether. This led to about 1% improvements in the val-\nidation accuracy, which is consistent with the view of\nBatch Normalization as a regularizer (Sec. 3.4): the ran-\ndomization inherent in our method should be most bene-\n\ufb01cial when it affects an example differently each time it is\nseen.\nReduce the photometric distortions.Because batch-\nnormalized networks train faster and observe each train-\ning example fewer times, we let the trainer focus on more\n\u201creal\u201d images by distorting them less.\n6", "mimetype": "text/plain", "start_char_idx": 2720, "end_char_idx": 5403, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "9e5da7c9-3d7a-492c-8536-da97ff2d4a4d": {"__data__": {"id_": "9e5da7c9-3d7a-492c-8536-da97ff2d4a4d", "embedding": null, "metadata": {"page_label": "7", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7ce62fc2-852d-4b9e-82e6-9e09e27b7d4a", "node_type": "4", "metadata": {"page_label": "7", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "1f559b6af1aeb191b732bad0f4d3ef7c1f1adc52a96e536924ecfaa1da20aa0e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebf3449a-18eb-4126-acfb-e9818a6013ea", "node_type": "1", "metadata": {}, "hash": "727ce4c39132d1d314a7a383a9140dce847e2b1f31650a5e5e2a6ac4fa3b17a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5M 10M 15M 20M 25M 30M\n0.4\n0.5\n0.6\n0.7\n0.8\nInception\nBN\u2212Baseline\nBN\u2212x5\nBN\u2212x30\nBN\u2212x5\u2212Sigmoid\nSteps to match Inception\nFigure 2:Single crop validation accuracy of Inception\nand its batch-normalized variants, vs. the number of\ntraining steps.\nModel Steps to 72.2% Max accuracy\nInception 31.0 \u00b7106 72.2%\nBN-Baseline 13.3 \u00b7106 72.7%\nBN-x5 2.1 \u00b7106 73.0%\nBN-x30 2.7 \u00b7106 74.8%\nBN-x5-Sigmoid 69.8%\nFigure 3:For Inception and the batch-normalized\nvariants, the number of training steps required to\nreach the maximum accuracy of Inception (72.2%),\nand the maximum accuracy achieved by the net-\nwork.\n4.2.2 Single-Network Classi\ufb01cation\nWe evaluated the following networks, all trained on the\nLSVRC2012 training data, and tested on the validation\ndata:\nInception: the network described at the beginning of\nSection 4.2, trained with the initial learning rate of 0.0015.\nBN-Baseline: Same as Inception with Batch Normal-\nization before each nonlinearity.\nBN-x5 : Inception with Batch Normalization and the\nmodi\ufb01cations in Sec. 4.2.1. The initial learning rate was\nincreased by a factor of 5, to 0.0075. The same learning\nrate increase with original Inception caused the model pa-\nrameters to reach machine in\ufb01nity.\nBN-x30 : LikeBN-x5 , but with the initial learning rate\n0.045 (30 times that of Inception).\nBN-x5-Sigmoid: LikeBN-x5 , but with sigmoid non-\nlinearityg(t) = 1\n1+exp(\u2212x) instead of ReLU. We also at-\ntempted to train the original Inception with sigmoid, but\nthe model remained at the accuracy equivalent to chance.\nIn Figure 2, we show the validation accuracy of the\nnetworks, as a function of the number of training steps.\nInception reached the accuracy of 72.2% after31 \u00b7106\ntraining steps. The Figure 3 shows, for each network,\nthe number of training steps required to reach the same\n72.2% accuracy, as well as the maximum validation accu-\nracy reached by the network and the number of steps to\nreach it.\nBy only using Batch Normalization (\nBN-Baseline), we\nmatch the accuracy of Inception in less than half the num-\nber of training steps. By applying the modi\ufb01cations in\nSec. 4.2.1, we signi\ufb01cantly increase the training speed of\nthe network.\nBN-x5 needs 14 times fewer steps than In-\nception to reach the 72.2% accuracy. Interestingly, in-\ncreasing the learning rate further (\nBN-x30 ) causes the\nmodel to train somewhatslowerinitially, but allows it to\nreach a higher \ufb01nal accuracy. It reaches 74.8% after6\u00b7106\nsteps, i.e. 5 times fewer steps than required by Inception\nto reach 72.2%.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2491, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ebf3449a-18eb-4126-acfb-e9818a6013ea": {"__data__": {"id_": "ebf3449a-18eb-4126-acfb-e9818a6013ea", "embedding": null, "metadata": {"page_label": "7", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7ce62fc2-852d-4b9e-82e6-9e09e27b7d4a", "node_type": "4", "metadata": {"page_label": "7", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "1f559b6af1aeb191b732bad0f4d3ef7c1f1adc52a96e536924ecfaa1da20aa0e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e5da7c9-3d7a-492c-8536-da97ff2d4a4d", "node_type": "1", "metadata": {"page_label": "7", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "05d5f50bee4070ceb4cc0ddd505f20f146fbd5c612083cdb466aba4f8ff5bf2c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5 times fewer steps than required by Inception\nto reach 72.2%.\nWe also veri\ufb01ed that the reduction in internal covari-\nate shift allows deep networks with Batch Normalization\nto be trained when sigmoid is used as the nonlinearity,\ndespite the well-known dif\ufb01culty of training such net-\nworks. Indeed,\nBN-x5-Sigmoid achieves the accuracy of\n69.8%. Without Batch Normalization, Inception with sig-\nmoid never achieves better than1/1000 accuracy.\n4.2.3 Ensemble Classi\ufb01cation\nThe current reported best results on the ImageNet Large\nScale Visual Recognition Competition are reached by the\nDeep Image ensemble of traditional models (Wu et al.,\n2015) and the ensemble model of (He et al., 2015). The\nlatter reports the top-5 error of 4.94%, as evaluated by the\nILSVRC server. Here we report a top-5 validation error of\n4.9%, and test error of 4.82% (according to the ILSVRC\nserver). This improves upon the previous best result, and\nexceeds the estimated accuracy of human raters according\nto (Russakovsky et al., 2014).\nFor our ensemble, we used 6 networks. Each was based\non\nBN-x30 , modi\ufb01ed via some of the following: increased\ninitial weights in the convolutional layers; using Dropout\n(with the Dropout probability of 5% or 10%, vs. 40%\nfor the original Inception); and using non-convolutional,\nper-activation Batch Normalization with last hidden lay-\ners of the model. Each network achieved its maximum\naccuracy after about6 \u00b7106 training steps. The ensemble\nprediction was based on the arithmetic average of class\nprobabilities predicted by the constituent networks. The\ndetails of ensemble and multicrop inference are similar to\n(Szegedy et al., 2014).\nWe demonstrate in Fig. 4 that batch normalization al-\nlows us to set new state-of-the-art by a healthy margin on\nthe ImageNet classi\ufb01cation challenge benchmarks.\n5 Conclusion\nWe have presented a novel mechanism for dramatically\naccelerating the training of deep networks. It is based on\nthe premise that covariate shift, which is known to com-\nplicate the training of machine learning systems, also ap-\n7", "mimetype": "text/plain", "start_char_idx": 2429, "end_char_idx": 4486, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "01423a09-f7ee-4d22-b061-a0a3e7c87731": {"__data__": {"id_": "01423a09-f7ee-4d22-b061-a0a3e7c87731", "embedding": null, "metadata": {"page_label": "8", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9a7586d2-94ad-4edf-bfae-fe267f8a4f58", "node_type": "4", "metadata": {"page_label": "8", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "9c01b8468ff620bdface81b3fba5c7426c7939f8f72f7061514266025a4a053e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07907d11-6a94-4299-bc61-fd3146cd3589", "node_type": "1", "metadata": {}, "hash": "8859015742bd62bcee8f43d093de874337b77fcac58fa1b363464a42525f88cf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Model Resolution Crops Models Top-1 error Top-5 error\nGoogLeNet ensemble 224 144 7 - 6.67%\nDeep Image low-res 256 - 1 - 7.96%\nDeep Image high-res 512 - 1 24.88 7.42%\nDeep Image ensemble variable - - - 5.98%\nBN-Inception single crop 224 1 1 25.2% 7.82%\nBN-Inception multicrop 224 144 1 21.99% 5.82%\nBN-Inception ensemble 224 144 6 20.1% 4.9% *\nFigure 4:Batch-Normalized Inception comparison with previous state of the art on the provided validation set com-\nprising 50000 images. *BN-Inception ensemble has reached 4.82% top-5 error on the 100000 images of the test set of\nthe ImageNet as reported by the test server.\nplies to sub-networks and layers, and removing it from\ninternal activations of the network may aid in training.\nOur proposed method draws its power from normalizing\nactivations, and from incorporating this normalization in\nthe network architecture itself. This ensures that the nor-\nmalization is appropriately handled by any optimization\nmethod that is being used to train the network. To en-\nable stochastic optimization methods commonly used in\ndeep network training, we perform the normalization for\neach mini-batch, and backpropagate the gradients through\nthe normalization parameters. Batch Normalization adds\nonly two extra parameters per activation, and in doing so\npreserves the representation ability of the network. We\npresented an algorithm for constructing, training, and per-\nforming inference with batch-normalized networks. The\nresulting networks can be trained with saturating nonlin-\nearities, are more tolerant to increased training rates, and\noften do not require Dropout for regularization.\nMerely adding Batch Normalization to a state-of-the-\nart image classi\ufb01cation model yields a substantial speedup\nin training. By further increasing the learning rates, re-\nmoving Dropout, and applying other modi\ufb01cations af-\nforded by Batch Normalization, we reach the previous\nstate of the art with only a small fraction of training steps\n\u2013 and then beat the state of the art in single-network image\nclassi\ufb01cation. Furthermore, by combining multiple mod-\nels trained with Batch Normalization, we perform better\nthan the best known system on ImageNet, by a signi\ufb01cant\nmargin.\nInterestingly, our method bears similarity to the stan-\ndardization layer of (G\u00a8 ulc \u00b8ehre & Bengio, 2013), though\nthe two methods stem from very different goals, and per-\nform different tasks. The goal of Batch Normalization\nis to achieve a stable distribution of activation values\nthroughout training, and in our experiments we apply it\nbefore the nonlinearity since that is where matching the\n\ufb01rst and second moments is more likely to result in a\nstable distribution. On the contrary, (G\u00a8 ulc \u00b8ehre & Bengio,\n2013) apply the standardization layer to theoutputof the\nnonlinearity, which results in sparser activations. In our\nlarge-scale image classi\ufb01cation experiments, we have not\nobserved the nonlinearityinputsto be sparse, neither with\nnor without Batch Normalization.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2978, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "07907d11-6a94-4299-bc61-fd3146cd3589": {"__data__": {"id_": "07907d11-6a94-4299-bc61-fd3146cd3589", "embedding": null, "metadata": {"page_label": "8", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9a7586d2-94ad-4edf-bfae-fe267f8a4f58", "node_type": "4", "metadata": {"page_label": "8", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "9c01b8468ff620bdface81b3fba5c7426c7939f8f72f7061514266025a4a053e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01423a09-f7ee-4d22-b061-a0a3e7c87731", "node_type": "1", "metadata": {"page_label": "8", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "1170069a23d6307bf32897f0e773bd88cf4dc542e5c7d5e165bcba356f628255", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Other notable differ-\nentiating characteristics of Batch Normalization include\nthe learned scale and shift that allow the BN transform\nto represent identity (the standardization layer did not re-\nquire this since it was followed by the learned linear trans-\nform that, conceptually, absorbs the necessary scale and\nshift), handling of convolutional layers, deterministic in-\nference that does not depend on the mini-batch, and batch-\nnormalizing each convolutional layer in the network.\nIn this work, we have not explored the full range of\npossibilities that Batch Normalization potentially enables.\nOur future work includes applications of our method to\nRecurrent Neural Networks (Pascanu et al., 2013), where\nthe internal covariate shift and the vanishing or exploding\ngradients may be especially severe, and which would al-\nlow us to more thoroughly test the hypothesis that normal-\nization improves gradient propagation (Sec. 3.3). We plan\nto investigate whether Batch Normalization can help with\ndomain adaptation, in its traditional sense \u2013 i.e. whether\nthe normalization performed by the network would al-\nlow it to more easily generalize to new data distribu-\ntions, perhaps with just a recomputation of the population\nmeans and variances (Alg. 2). Finally, we believe that fur-\nther theoretical analysis of the algorithm would allow still\nmore improvements and applications.\nReferences\nBengio, Yoshua and Glorot, Xavier. Understanding the\ndif\ufb01culty of training deep feedforward neural networks.\nInProceedings of AISTATS 2010, volume 9, pp. 249\u2013\n256, May 2010.\nDean, Jeffrey, Corrado, Greg S., Monga, Rajat, Chen, Kai,\nDevin, Matthieu, Le, Quoc V ., Mao, Mark Z., Ranzato,\nMarc\u2019Aurelio, Senior, Andrew, Tucker, Paul, Yang, Ke,\nand Ng, Andrew Y . Large scale distributed deep net-\nworks. InNIPS, 2012.\nDesjardins, Guillaume and Kavukcuoglu, Koray. Natural\nneural networks. (unpublished).\nDuchi, John, Hazan, Elad, and Singer, Yoram. Adaptive\nsubgradient methods for online learning and stochastic\n8", "mimetype": "text/plain", "start_char_idx": 2979, "end_char_idx": 4984, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "cbae0c2f-3bd0-4689-872d-814147990287": {"__data__": {"id_": "cbae0c2f-3bd0-4689-872d-814147990287", "embedding": null, "metadata": {"page_label": "9", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "07f60d65-1beb-412a-84d1-4819585ce3be", "node_type": "4", "metadata": {"page_label": "9", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "1294a7884c696aa40d9d74ef12519007c2a511e416f34e05fce9ba537740502f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4981ce5-8c19-48e8-8416-7381284b32d7", "node_type": "1", "metadata": {}, "hash": "6681446544e2ff903de9769a249a085ee7890b490c49644231116fda6413b886", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "optimization.J. Mach. Learn. Res., 12:2121\u20132159, July\n2011. ISSN 1532-4435.\nG\u00a8 ulc \u00b8ehre, C \u00b8 aglar and Bengio, Yoshua. Knowledge mat-\nters: Importance of prior information for optimization.\nCoRR, abs/1301.4083, 2013.\nHe, K., Zhang, X., Ren, S., and Sun, J. Delving Deep\ninto Recti\ufb01ers: Surpassing Human-Level Performance\non ImageNet Classi\ufb01cation.ArXiv e-prints, February\n2015.\nHyv\u00a8 arinen, A. and Oja, E. Independent component anal-\nysis: Algorithms and applications.Neural Netw., 13\n(4-5):411\u2013430, May 2000.\nJiang, Jing. A literature survey on domain adaptation of\nstatistical classi\ufb01ers, 2008.\nLeCun, Y ., Bottou, L., Bengio, Y ., and Haffner, P.\nGradient-based learning applied to document recog-\nnition.Proceedings of the IEEE, 86(11):2278\u20132324,\nNovember 1998a.\nLeCun, Y ., Bottou, L., Orr, G., and Muller, K. Ef\ufb01cient\nbackprop. In Orr, G. and K., Muller (eds.),Neural Net-\nworks: Tricks of the trade. Springer, 1998b.\nLyu, S and Simoncelli, E P. Nonlinear image representa-\ntion using divisive normalization. InProc. Computer\nVision and Pattern Recognition, pp. 1\u20138. IEEE Com-\nputer Society, Jun 23-28 2008. doi: 10.1109/CVPR.\n2008.4587821.\nNair, Vinod and Hinton, Geoffrey E. Recti\ufb01ed linear units\nimprove restricted boltzmann machines. InICML , pp.\n807\u2013814. Omnipress, 2010.\nPascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua.\nOn the dif\ufb01culty of training recurrent neural networks.\nInProceedings of the 30th International Conference on\nMachine Learning, ICML 2013, Atlanta, GA, USA, 16-\n21 June 2013, pp. 1310\u20131318, 2013.\nPovey, Daniel, Zhang, Xiaohui, and Khudanpur, San-\njeev. Parallel training of deep neural networks with\nnatural gradient and parameter averaging.CoRR ,\nabs/1410.7455, 2014.\nRaiko, Tapani, Valpola, Harri, and LeCun, Yann. Deep\nlearning made easier by linear transformations in per-\nceptrons. InInternational Conference on Arti\ufb01cial In-\ntelligence and Statistics (AISTATS), pp. 924\u2013932, 2012.\nRussakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan,\nSatheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpa-\nthy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg,\nAlexander C., and Fei-Fei, Li. ImageNet Large Scale\nVisual Recognition Challenge, 2014.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2175, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a4981ce5-8c19-48e8-8416-7381284b32d7": {"__data__": {"id_": "a4981ce5-8c19-48e8-8416-7381284b32d7", "embedding": null, "metadata": {"page_label": "9", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "07f60d65-1beb-412a-84d1-4819585ce3be", "node_type": "4", "metadata": {"page_label": "9", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "1294a7884c696aa40d9d74ef12519007c2a511e416f34e05fce9ba537740502f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbae0c2f-3bd0-4689-872d-814147990287", "node_type": "1", "metadata": {"page_label": "9", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "f16a93184a449c3a3c0c3b2590fb1f1a38c660a3ae585eb61599af66cea290ae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ImageNet Large Scale\nVisual Recognition Challenge, 2014.\nSaxe, Andrew M., McClelland, James L., and Ganguli,\nSurya. Exact solutions to the nonlinear dynamics\nof learning in deep linear neural networks.CoRR ,\nabs/1312.6120, 2013.\nShimodaira, Hidetoshi. Improving predictive inference\nunder covariate shift by weighting the log-likelihood\nfunction.Journal of Statistical Planning and Inference,\n90(2):227\u2013244, October 2000.\nSrivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex,\nSutskever, Ilya, and Salakhutdinov, Ruslan. Dropout:\nA simple way to prevent neural networks from over\ufb01t-\nting.J. Mach. Learn. Res., 15(1):1929\u20131958, January\n2014.\nSutskever, Ilya, Martens, James, Dahl, George E., and\nHinton, Geoffrey E. On the importance of initial-\nization and momentum in deep learning. InICML\n(3), volume 28 ofJMLR Proceedings, pp. 1139\u20131147.\nJMLR.org, 2013.\nSzegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet,\nPierre, Reed, Scott, Anguelov, Dragomir, Erhan, Du-\nmitru, Vanhoucke, Vincent, and Rabinovich, An-\ndrew. Going deeper with convolutions.CoRR ,\nabs/1409.4842, 2014.\nWiesler, Simon and Ney, Hermann. A convergence anal-\nysis of log-linear training. In Shawe-Taylor, J., Zemel,\nR.S., Bartlett, P., Pereira, F.C.N., and Weinberger, K.Q.\n(eds.),Advances in Neural Information Processing Sys-\ntems 24, pp. 657\u2013665, Granada, Spain, December 2011.\nWiesler, Simon, Richard, Alexander, Schl\u00a8 uter, Ralf, and\nNey, Hermann. Mean-normalized stochastic gradient\nfor large-scale deep learning. InIEEE International\nConference on Acoustics, Speech, and Signal Process-\ning, pp. 180\u2013184, Florence, Italy, May 2014.\nWu, Ren, Yan, Shengen, Shan, Yi, Dang, Qingqing, and\nSun, Gang. Deep image: Scaling up image recognition,\n2015.\nAppendix\nVariant of the Inception Model Used\nFigure 5 documents the changes that were performed\ncompared to the architecture with respect to the\nGoogleNet archictecture. For the interpretation of this\ntable, please consult (Szegedy et al., 2014). The notable\narchitecture changes compared to the GoogLeNet model\ninclude:\n\u2022The 5\u00d75 convolutional layers are replaced by two\nconsecutive 3\u00d73 convolutional layers. This in-\ncreases the maximum depth of the network by 9\n9", "mimetype": "text/plain", "start_char_idx": 2119, "end_char_idx": 4306, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1b897baa-9354-4eee-9132-5bf7e6f2d151": {"__data__": {"id_": "1b897baa-9354-4eee-9132-5bf7e6f2d151", "embedding": null, "metadata": {"page_label": "10", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7874f0d5-175f-4356-b007-f4f1582ddcb0", "node_type": "4", "metadata": {"page_label": "10", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "5331b0db739e16afdc7f98cc00f43efcc9d86517c1dd9795f6cc8f5e101c466b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "weight layers. Also it increases the number of pa-\nrameters by 25% and the computational cost is in-\ncreased by about 30%.\n\u2022The number 28\u00d728 inception modules is increased\nfrom 2 to 3.\n\u2022Inside the modules, sometimes average, sometimes\nmaximum-pooling is employed. This is indicated in\nthe entries corresponding to the pooling layers of the\ntable.\n\u2022There are no across the board pooling layers be-\ntween any two Inception modules, but stride-2 con-\nvolution/pooling layers are employed before the \ufb01l-\nter concatenation in the modules 3c, 4e.\nOur model employed separable convolution with depth\nmultiplier8 on the \ufb01rst convolutional layer. This reduces\nthe computational cost while increasing the memory con-\nsumption at training time.\n10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 736, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "6a2d5ae9-72f7-4331-b3aa-b6f8b4638445": {"__data__": {"id_": "6a2d5ae9-72f7-4331-b3aa-b6f8b4638445", "embedding": null, "metadata": {"page_label": "11", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1c939bf8-36c6-471f-beb2-804a4b9be401", "node_type": "4", "metadata": {"page_label": "11", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}, "hash": "6f5ebda13504d376d02007c2044990bf5d1573859beaee97090e648b6b16916d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "type patch size/\nstride\noutput\nsize depth #1\u00d7 1 #3\u00d7 3\nreduce #3\u00d7 3 double #3\u00d7 3\nreduce\ndouble\n#3\u00d7 3 Pool +proj\nconvolution* 7\u00d7 7/2 112\u00d7 112\u00d7 64 1\nmax pool 3\u00d7 3/2 56\u00d7 56\u00d7 64 0\nconvolution 3\u00d7 3/1 56\u00d7 56\u00d7 192 1 64 192\nmax pool 3\u00d7 3/2 28\u00d7 28\u00d7 192 0\ninception (3a) 28\u00d7 28\u00d7 256 3 64 64 64 64 96 avg + 32\ninception (3b) 28\u00d7 28\u00d7 320 3 64 64 96 64 96 avg + 64\ninception (3c) stride 2 28\u00d7 28\u00d7 576 3 0 128 160 64 96 max + pass through\ninception (4a) 14\u00d7 14\u00d7 576 3 224 64 96 96 128 avg + 128\ninception (4b) 14\u00d7 14\u00d7 576 3 192 96 128 96 128 avg + 128\ninception (4c) 14\u00d7 14\u00d7 576 3 160 128 160 128 160 avg + 128\ninception (4d) 14\u00d7 14\u00d7 576 3 96 128 192 160 192 avg + 128\ninception (4e) stride 2 14\u00d7 14\u00d7 1024 3 0 128 192 192 256 max + pass through\ninception (5a) 7\u00d7 7\u00d7 1024 3 352 192 320 160 224 avg + 128\ninception (5b) 7\u00d7 7\u00d7 1024 3 352 192 320 192 224 max + 128\navg pool 7\u00d7 7/1 1\u00d7 1\u00d7 1024 0\nFigure 5: Inception architecture\n11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 910, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"ff120415-d091-4aba-af3b-5ef07e4acfe8": {"node_ids": ["4b024e4f-9516-4c4e-bea0-6f8d6fb97c4f", "5703dfa2-36f8-4bd9-aed9-461a67bfba2d"], "metadata": {"page_label": "1", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}}, "51867cd4-3fe9-4032-abba-482d2ec2a5af": {"node_ids": ["f82a26dd-9a89-473c-a434-3e9065e005d0", "5bbd3a55-1c50-45a2-8f04-73868fde21b3"], "metadata": {"page_label": "2", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}}, "d30862d0-3f80-4ee5-a0c8-f9675fd1d611": {"node_ids": ["ae445486-cbdf-4591-b90c-9e3627c1f240", "2a07ad9b-cc01-4e63-be9b-622d384053e1"], "metadata": {"page_label": "3", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}}, "45e734e1-7ebc-4b90-bd38-ca7f431f6c65": {"node_ids": ["d49300ea-71ad-4833-a975-f52f92d82262", "56256e38-000c-40eb-9909-744850a328a0", "f2dd1dd6-71f7-4c99-9c1b-3b44ef80881a"], "metadata": {"page_label": "4", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}}, "9be60697-4e3e-4a43-b0c2-722eef472308": {"node_ids": ["3b443e15-85f4-48bf-a8a5-8da7273a185d", "791cd122-ad36-465b-99d6-042e1a7b8358"], "metadata": {"page_label": "5", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}}, "b54fdd17-312e-426d-bec6-9602ae4b50d2": {"node_ids": ["edc6e3cc-e431-4806-97a6-bbd33c4fd6ad", "c5f984af-ed55-453c-9193-63b39bfce16c"], "metadata": {"page_label": "6", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}}, "7ce62fc2-852d-4b9e-82e6-9e09e27b7d4a": {"node_ids": ["9e5da7c9-3d7a-492c-8536-da97ff2d4a4d", "ebf3449a-18eb-4126-acfb-e9818a6013ea"], "metadata": {"page_label": "7", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}}, "9a7586d2-94ad-4edf-bfae-fe267f8a4f58": {"node_ids": ["01423a09-f7ee-4d22-b061-a0a3e7c87731", "07907d11-6a94-4299-bc61-fd3146cd3589"], "metadata": {"page_label": "8", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}}, "07f60d65-1beb-412a-84d1-4819585ce3be": {"node_ids": ["cbae0c2f-3bd0-4689-872d-814147990287", "a4981ce5-8c19-48e8-8416-7381284b32d7"], "metadata": {"page_label": "9", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}}, "7874f0d5-175f-4356-b007-f4f1582ddcb0": {"node_ids": ["1b897baa-9354-4eee-9132-5bf7e6f2d151"], "metadata": {"page_label": "10", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}}, "1c939bf8-36c6-471f-beb2-804a4b9be401": {"node_ids": ["6a2d5ae9-72f7-4331-b3aa-b6f8b4638445"], "metadata": {"page_label": "11", "file_name": "batch-normalisation.pdf", "file_path": "C:\\Users\\eh5cd\\OneDrive\\Desktop\\qa system\\Data\\batch-normalisation.pdf", "file_type": "application/pdf", "file_size": 173548, "creation_date": "2025-02-20", "last_modified_date": "2025-02-20"}}}}